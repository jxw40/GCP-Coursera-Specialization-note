{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ML on GCP C7] Serving ML Predictions in batch and real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Duration is 1 min\n",
    "\n",
    "In this lab, you run Dataflow pipelines to serve predictions for batch requests as well as streaming in real-time.\n",
    "\n",
    "### What you learn\n",
    "In this lab, you write code to:\n",
    "\n",
    "* Create a prediction service that calls your trained model deployed in Cloud to serve predictions\n",
    "\n",
    "* Run a Dataflow job to have the prediction service read in batches from a CSV file and serve predictions\n",
    "\n",
    "* Run a streaming Dataflow pipeline to read requests real-time from Cloud Pub/Sub and write predictions into a BigQuery table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Cloud Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy trained model\n",
    "### Step 1\n",
    "Set necessary variables and create a bucket:\n",
    "```\n",
    "REGION=us-central1\n",
    "BUCKET=$(gcloud config get-value project)\n",
    "TFVERSION=1.7\n",
    "gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "```\n",
    "\n",
    "### Step 2\n",
    "Copy trained model into your bucket:\n",
    "```\n",
    "gsutil -m cp -R gs://cloud-training-demos/babyweight/trained_model gs://${BUCKET}/babyweight\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy trained model\n",
    "### Step 1\n",
    "Set necessary variables:\n",
    "```\n",
    "MODEL_NAME=babyweight\n",
    "MODEL_VERSION=ml_on_gcp\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/export/exporter/ | tail -1)\n",
    "```\n",
    "\n",
    "### Step 2\n",
    "Deploy trained model:\n",
    "```\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse lab files\n",
    "Duration is 5 min\n",
    "\n",
    "### Step 1\n",
    "Clone the course repository:\n",
    "```\n",
    "cd ~\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "```\n",
    "\n",
    "### Step 2\n",
    "In Cloud Shell, navigate to the folder containing the code for this lab:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving\n",
    "```\n",
    "\n",
    "### Step 3\n",
    "Run the what_to_fix.sh script to see a list of items you need to add/modify to existing code to run your app:\n",
    "```\n",
    "./what_to_fix.sh\n",
    "```\n",
    "As a result of this, you will see a list of filenames and lines within those files marked with TODO. These are the lines where you have to add/modify code. For this lab, you will focus on #TODO items for .java files only, namely BabyweightMLService.java : which is your prediction service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the code is organized\n",
    "![](f2aeed7941e38072.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction service\n",
    "In this section, you fix the code in BabyweightMLService.java and test it with the run_once.sh script that is provided. If you need help with the code, look at the next section that provides hints on how to fix code in BabyweightMLService.java.\n",
    "\n",
    "### Step 1\n",
    "You may use the Cloud Shell code editor to view and edit the contents of these files.\n",
    "\n",
    "Click on the (b8ebde10ba2a31c8.png) icon on the top right of your Cloud Shell window to launch Code Editor.\n",
    "\n",
    "### Step 2\n",
    "After it is launched, navigate to the following directory: training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving/pipeline/src/main/java/com/google/cloud/training/mlongcp\n",
    "\n",
    "### Step 3\n",
    "Open the BabyweightMLService.java files and replace #TODOs in the code.\n",
    "\n",
    "### Step 4\n",
    "Once completed, go into your Cloud Shell and run the run_once.sh script to test your ML service.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving\n",
    "./run_once.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve predictions for batch requests\n",
    "This section of the lab calls AddPrediction.java that takes a batch input (one big CSV), calls the prediction service to generate baby weight predictions and writes them into local files (multiple CSVs).\n",
    "\n",
    "### Step 1\n",
    "In your Cloud Shell code editor, open the AddPrediction.java file available in the following directory: training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving/pipeline/src/main/java/com/google/cloud/training/mlongcp\n",
    "\n",
    "### Step 2\n",
    "Look through the code and notice how, based on input argument, it decides to set up a batch or streaming pipeline, and creates the appropriate TextInputOutput or PubSubBigQuery io object respectively to handle the reading and writing.\n",
    "\n",
    "> Note: Look back at the diagram in \"how code is organized\" section to make sense of it all.\n",
    "\n",
    "### Step 3\n",
    "Test batch mode by running the run_ontext.sh script provided in the lab directory:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving\n",
    "./run_ontext.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve predictions real-time with a streaming pipeline\n",
    "In this section of the lab, you will launch a streaming pipeline with Dataflow, which will accept incoming information from Cloud Pub/Sub, use the info to call the prediction service to get baby weight predictions, and finally write that info into a BigQuery table.\n",
    "\n",
    "### Step 1\n",
    "On your GCP Console's left-side menu, go into Pub/Sub and click the CREATE TOPIC button on top. Create a topic called babies.\n",
    "\n",
    "c4128dad787aaada.png\n",
    "\n",
    "### Step 2\n",
    "Back in your Cloud Shell, modify the script `__run_dataflow.sh__` to get Project ID (using --project) from command line arguments, and then run as follows:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving\n",
    "./run_dataflow.sh\n",
    "```\n",
    "This will create a streaming Dataflow pipeline.\n",
    "\n",
    "### Step 3\n",
    "Back in your GCP Console, use the left-side menu to go into Dataflow and verify that the streaming job is created.\n",
    "\n",
    "eaf7891a8d680d8e.png\n",
    "\n",
    "### Step 4\n",
    "Next, click on the job name to view the pipeline graph. Click on the pipeline steps (boxes) and look at the run details (like system lag, elements added, etc.) of that step on the right side.\n",
    "\n",
    "662fb484741d22e2.png\n",
    "\n",
    "This means that your pipeline is running and waiting for input. Let's provide input through the Pub/Sub topic.\n",
    "\n",
    "### Step 5\n",
    "Copy some lines from your example.csv.gz:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/machine_learning/deepdive/06_structured/labs/serving\n",
    "zcat exampledata.csv.gz\n",
    "```\n",
    "\n",
    "### Step 6\n",
    "On your GCP Console, go back into Pub/Sub, click on the babies topic, and then click on Publish message button on top. In the message box, paste the lines you just copied from exampledata.csv.gz and click on Publish button.\n",
    "\n",
    "9e58fd14886fba1f.png\n",
    "\n",
    "### Step 7\n",
    "You may go back into Dataflow jobs on your GCP Console, click on your job and see how the run details have changed for the steps, for example click on write_toBQ and look at Elements added.\n",
    "\n",
    "### Step 8\n",
    "Lets verify that the predicted weights have been recorded into the BigQuery table.\n",
    "\n",
    "Open the BigQuery ‘Classic’ console UI in a new tab: BigQuery\n",
    "You may be prompted to enter your lab account’s password again.\n",
    "\n",
    "Note: Another way to open BigQuery is to open the navigation menu (HorizontalLine) in the GCP console and then click the BigQuery link. This will open the new BigQuery UI, which is currently in Beta. For these labs we will be using the Classic UI, which you can access either by opening to the link given above or by selecting Go to Classic UI.\n",
    "\n",
    "Google_choose_Account\n",
    "\n",
    "The BigQuery console appears:\n",
    "\n",
    "Google_choose_Account\n",
    "\n",
    "Ensure that BigQuery is set to your qwiklabs-gcp-******** project. If it is, then proceed to the next step. If it’s not set to your project (for example, it might be set to another project like Qwiklabs Resources), click the drop down arrow next to the project name and then click Switch to project, then select your project as shown below:\n",
    "Google_choose_Account\n",
    "\n",
    "Look at the left-side menu and you should see the babyweight dataset. Click on the blue down arrow to its left, and you should see your prediction table.\n",
    "\n",
    "> Note: If you do not see the prediction table, give it a few minutes as the pipeline has allowed-latency and that can add some delay.\n",
    "\n",
    "1fbaf89946687844.png\n",
    "\n",
    "### Step 9\n",
    "Click on Compose Query button on the top left. Type the query below in the query box to retrieve rows from your predictions table. Click on Show Options button under the query box and uncheck Use Legacy SQL and click Hide Options.\n",
    "```\n",
    "SELECT * FROM babyweight.predictions LIMIT 1000\n",
    "```\n",
    "ccb8ccce73d92d9a.png\n",
    "\n",
    "### Step 10\n",
    "Click the Run Query button. Notice the predicted_weights_pounds column in the result.\n",
    "\n",
    "549d498cd2f18780.png\n",
    "\n",
    "### Step 11\n",
    "Remember that your pipeline is still running. You can publish additional messages from your example.csv.gz and verify new rows added to your predictions table. Once you are satisfied, you may stop the Dataflow pipeline by going into your Dataflow Jobs page, and click the Stop job button on the right side Job summary window.\n",
    "\n",
    "69cff18d8f1cabb5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End your lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/06_structured/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
