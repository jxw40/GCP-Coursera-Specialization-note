{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for sparsity\n",
    "\n",
    "**Zeroing out coefficients can help with performance, especially with large models and sparse inputs**\n",
    "\n",
    "|Action|Impact|\n",
    "|:-|:-|\n",
    "|Fewer coefficients to store/load|Reduce memory,model,size|\n",
    "|Fewer multiplications needed|Increase prediction speed|\n",
    "\n",
    "L2 regularization only makes weights small, not zero\n",
    "\n",
    "Feature crosses lead to lots of input nodes, so having zero weights is especially important\n",
    "\n",
    "L0-norm(the count of non-zero weights) is an NP-hard, non-convex optimization problem\n",
    "\n",
    "![](L1_norm.png)\n",
    "\n",
    "**Elastic nets combine the feature selection of L1 regularization with the generalizability of L2 regularization**\n",
    "\n",
    "This way, you get the benefits of sparsity for really poor predictive features while also keeping decent and great features with smaller weights to provide a good generalization.\n",
    "\n",
    "#### L1 Regularization Quiz\n",
    "\n",
    "What does L1 regularization tends to do to a model's low predictive features' parameter weights?\n",
    "\n",
    "**C. Have zero values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: L1 Regularization\n",
    "\n",
    "https://goo.gl/281mPF\n",
    "\n",
    "Try training with and without L1 regularization. What’s the difference?\n",
    "\n",
    "regularization=L1 dataset=circle\n",
    "![](regularization=L1_dataset=circle.png)\n",
    "\n",
    "### Lab Solution: L1 Regularization\n",
    "![](lab_L1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "![](logistic_regression.png)\n",
    "\n",
    "**The output of Logistic Regression is a calibrated probability estimate**\n",
    "\n",
    "the sigmoid function is the cumulative distribution function of the logistic probability distribution whose quantile function is the inverse of the logit which models the log odds\n",
    "\n",
    "Useful because we can cast binary classification problems into probabilistic problems\n",
    "\n",
    "![](cross-entropy.png)\n",
    "![](regularization_is_important.png)\n",
    "![](logistic_regression_regularization_quiz.png)\n",
    "\n",
    "**Often we do both regularization and early stopping to counteract overfitting**\n",
    "\n",
    "In many real-world problems, the probabilty is not enough; we need to make a binary decision\n",
    "\n",
    "![](ROC_curve.png)\n",
    "To create a curve, we would pick each possible decision threshold and re-evaluate. Each threshold value creates a single point but by evaluating many thresholds eventually a curve is formed.\n",
    "\n",
    "Each model would create a different ROC curve. How can  we use these curves to compare relative performance of our models when we don't know exactly what decision threshold we want to use?\n",
    "![](AUC.png)\n",
    "\n",
    "#### Logistic Regression predictions should be unbiased\n",
    "\n",
    "**average of predictions == average of observations**\n",
    "\n",
    "Look for bias in slices of data, this can guide improvements\n",
    "![](bucketed_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Combine features as  an alternative to feature crossing\n",
    "![](adding_non_linearity.png)\n",
    "\n",
    "#### Non-linearity Quiz\n",
    "\n",
    "Why is it important adding non-linear activation functions to neural networks?\n",
    "\n",
    "** Stops the layers from collapsing back into just a linear model**\n",
    "\n",
    "Our favorite non-linearity is the Rectified Linear Unit\n",
    "\n",
    "There are many different ReLU variants\n",
    "\n",
    "\n",
    "#### Neural Network Complexity Quiz\n",
    "\n",
    "Neural networks can be arbitrarily complex. To increase hidden dimensions, I can add _______. To increase function composition, I can add _______. If I have multiple labels per example, I can add _______.\n",
    "\n",
    "**Neurons, layers, outputs**\n",
    "\n",
    "### Lab: Neural Networks Playground\n",
    "\n",
    "https://goo.gl/2eig4q\n",
    "\n",
    "https://goo.gl/wXbGDW\n",
    "\n",
    "https://goo.gl/i9r55D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks\n",
    "\n",
    "![](DNNRegressor.png)\n",
    "![](three_common_failure.png)\n",
    "\n",
    "#### Gradient Descent Debugging Quiz\n",
    "\n",
    "Which of these is good advice if my model is experiencing exploding gradients?\n",
    "\n",
    "Lower the learning rate\n",
    "\n",
    "Add weight regularization\n",
    "\n",
    "Add gradient clipping\n",
    "\n",
    "Add batch normalization\n",
    "\n",
    "See a doctor\n",
    "\n",
    "Both C and D\n",
    "\n",
    "A,C,D\n",
    "\n",
    "**A,B,C,D**\n",
    "\n",
    "\n",
    "![](dropout.png)\n",
    "\n",
    "Dropout simulates emsemble learning\n",
    "\n",
    "#### Dropout Quiz\n",
    "\n",
    "Dropout acts as another form of ______. It forces data to flow down ______ paths so that there is a more even spread. It also simulates ______ learning. Don’t forget to scale the dropout activations by the inverse of the ______. We remove dropout during ______.\n",
    "\n",
    "**Regularization, multiple, ensemble, keep probability, inference**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-class Neural Networks\n",
    "\n",
    "Use one softmax loss for all possible classes\n",
    "```\n",
    "logits = tf.matmul(...)  # shape = [batch_size, num_classes]\n",
    "labels = ...             # one-hot encoding in [0, num_classes)\n",
    "                         # shape = [batch_size, num_classes]\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(  # shape = [batch_size]\n",
    "        logits, labels)   \n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "logits = tf.matmul(...)  # shape = [batch_size, num_classes]\n",
    "labels = ...             # index in [0, num_classes]\n",
    "                         # shape = [batch_size]\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits_v2(  # shape = [batch_size]\n",
    "        logits, labels)   \n",
    ")\n",
    "```\n",
    "\n",
    "Use softmax only when classes are mutually exclusive.\n",
    "\"Multi-Class, Single-Label Classification\". An example may be a member of only one class\n",
    "\n",
    "Are there multi-class where examples may **belong to more than one class**\n",
    "```\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits, labels)        # shape = [batch_size, num_classes]\n",
    "```\n",
    "\n",
    "**If you have hundreds or thousands of classes, loss computation can become a significant bottleneck.** Need to evaluate every output node for every example\n",
    "![](nce.png)\n",
    "\n",
    "#### Softmax Quiz\n",
    "\n",
    "For our classification output, if we have both mutually exclusive labels and probabilities, we should use ______. If the labels are mutually exclusive, but the probabilities aren’t, we should use ______. If our labels aren’t mutually exclusive, we should use ______.\n",
    "\n",
    "I. tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "II. tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "III. tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "\n",
    "**III, II, I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
