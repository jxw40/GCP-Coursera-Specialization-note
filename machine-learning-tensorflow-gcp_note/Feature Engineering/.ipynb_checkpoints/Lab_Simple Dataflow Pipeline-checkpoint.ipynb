{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dataflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Duration is 1 min\n",
    "\n",
    "In this lab, you learn how to write a simple Dataflow pipeline and run it both locally and on the cloud.\n",
    "\n",
    "### What you learn\n",
    "\n",
    "In this lab, you learn how to:\n",
    "\n",
    "* Write a simple pipeline in Python\n",
    "\n",
    "* Execute the query on the local machine\n",
    "\n",
    "* Execute the query on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Duration is 1 min\n",
    "\n",
    "The goal of this lab is to become familiar with the structure of a Dataflow project and learn how to execute a Dataflow pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Dataflow project\n",
    "\n",
    "Duration is 3 min\n",
    "\n",
    "### Step 1\n",
    "Start CloudShell and clone the source repo which has starter scripts for this lab:\n",
    "```\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "```\n",
    "Then navigate to the code for this lab:\n",
    "```\n",
    "cd training-data-analyst/courses/data_analysis/lab2/python\n",
    "```\n",
    "\n",
    "### Step 2\n",
    "Install the necessary dependencies for Python dataflow:\n",
    "```\n",
    "sudo ./install_packages.sh\n",
    "```\n",
    "Verify that you have the right version of pip (should be > 8.0):\n",
    "```\n",
    "pip -V\n",
    "```\n",
    "If not, open a new CloudShell tab and it should pick up the updated pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline filtering\n",
    "\n",
    "Duration is 5 min\n",
    "\n",
    "### Step 1\n",
    "View the source code for the pipeline using the Cloud Shell file browser:\n",
    "\n",
    "f1f5da1fd2c75d3a.png\n",
    "\n",
    "In the file directory, navigate to /training-data-analyst/courses/data_analysis/lab2/python.\n",
    "\n",
    "499badba3c564a51.png\n",
    "\n",
    "Find grep.py.\n",
    "\n",
    "8c6f80d2b0a9f0d3.png\n",
    "\n",
    "Or you can navigate to the directly and view the file using nano if you prefer:\n",
    "```\n",
    "nano grep.py\n",
    "```\n",
    "\n",
    "### Step 2\n",
    "What files are being read? _____________*.java________________________________________\n",
    "\n",
    "What is the search term? _______________import_______________________________________\n",
    "\n",
    "Where does the output go? _________/tmp/output_____________________________________\n",
    "\n",
    "There are three transforms in the pipeline:\n",
    "\n",
    "What does the transform do? _________________________________\n",
    "\n",
    "What does the second transform do? ______________________________\n",
    "\n",
    "Where does its input come from? _____training-data-analyst/courses/data_analysis/lab2/javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/\n",
    "___________________\n",
    "\n",
    "What does it do with this input? ___________find all line starts with \"import\"_______________\n",
    "\n",
    "What does it write to its output? __________________________\n",
    "\n",
    "Where does the output go to? ____________________________\n",
    "\n",
    "What does the third transform do? _____________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline locally\n",
    "Duration is 2 min\n",
    "\n",
    "### Step 1\n",
    "Execute locally:\n",
    "```\n",
    "python grep.py\n",
    "```\n",
    "Note: if you see an error that says \"No handlers could be found for logger \"oauth2client.contrib.multistore_file\", you may ignore it. The error is simply saying that logging from the oauth2 library will go to stderr.\n",
    "\n",
    "### Step 2\n",
    "Examine the output file:\n",
    "```\n",
    "cat /tmp/output-*\n",
    "```\n",
    "Does the output seem logical? ______________________\n",
    "\n",
    "```\n",
    "import java.time.Instant;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n",
    "import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.Sum;\n",
    "import org.apache.beam.sdk.transforms.windowing.SlidingWindows;\n",
    "import org.apache.beam.sdk.transforms.windowing.Window;\n",
    "import org.joda.time.Duration;\n",
    "import com.google.api.services.bigquery.model.TableFieldSchema;\n",
    "import com.google.api.services.bigquery.model.TableRow;\n",
    "import com.google.api.services.bigquery.model.TableSchema;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.Sum;\n",
    "import org.apache.beam.sdk.transforms.Top;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import java.util.ArrayList;\n",
    "import java.util.Collections;\n",
    "import java.util.List;\n",
    "import java.util.Map;\n",
    "import com.google.api.services.bigquery.model.TableRow;\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.Sum;\n",
    "import org.apache.beam.sdk.transforms.Top;\n",
    "import org.apache.beam.sdk.transforms.View;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.PCollectionView;\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline on the cloud\n",
    "Duration is 10 min\n",
    "\n",
    "### Step 1\n",
    "If you don't already have a bucket on Cloud Storage, create one from the Storage section of the GCP console. Bucket names have to be globally unique.\n",
    "\n",
    "### Step 2\n",
    "Copy some Java files to the cloud (make sure to replace <YOUR-BUCKET-NAME> with the bucket name you created in the previous step):\n",
    "```\n",
    "gsutil cp ../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java gs://<YOUR-BUCKET-NAME>/javahelp\n",
    "```\n",
    "\n",
    "### Step 3\n",
    "Edit the Dataflow pipeline in grepc.py by opening up in the Cloud Shell in-browser editor again or by using the command line with nano:\n",
    "\n",
    "2267f36fb97f67cc.png\n",
    "```\n",
    "nano grepc.py\n",
    "```\n",
    "and changing the PROJECT and BUCKET variables appropriately.\n",
    "\n",
    "### Step 4\n",
    "Submit the Dataflow to the cloud:\n",
    "```\n",
    "python grepc.py\n",
    "```\n",
    "Because this is such a small job, running on the cloud will take significantly longer than running it locally (on the order of 2-3 minutes).\n",
    "\n",
    "### Step 5\n",
    "On your Cloud Console, navigate to the Dataflow section (from the 3 bars on the top-left menu), and look at the Jobs. Select your job and monitor its progress. You will see something like this:\n",
    "\n",
    "f55e71303e86b156.png\n",
    "\n",
    "### Step 6\n",
    "Wait for the job status to turn to Succeeded. At this point, your CloudShell will display a command-line prompt. In CloudShell, examine the output:\n",
    "```\n",
    "gsutil cat gs://<YOUR-BUCKET-NAME>/javahelp/output-*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you learned\n",
    "\n",
    "Duration is 1 min\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "* Executed a Dataflow pipeline locally\n",
    "* Executed a Dataflow pipeline on the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End your lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
