{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Time-Windowed Features in Cloud Dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lab you will ingest, transform, and analyze a taxi cab dataset using Google Cloud Dataprep. We will calculate key reporting metrics like the average number of passengers picked up in the past hour.\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "\n",
    "* Build a new Flow using Cloud Dataprep\n",
    "* Create and chain transformation steps with recipes\n",
    "* Running Cloud Dataprep jobs (Dataflow behind-the-scenes)\n",
    "\n",
    "Cloud Dataprep is Google's self-service data preparation tool. In this lab, you will learn how to clean and enrich multiple datasets using Cloud Dataprep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new Storage Bucket\n",
    "Skip this section if you already have a GCS Bucket\n",
    "\n",
    "### Step 1\n",
    "Open the Google Cloud Console at console.cloud.google.com.\n",
    "\n",
    "### Step 2\n",
    "Go to Storage in the Navigation menu (left-side navigation).\n",
    "\n",
    "### Step 3\n",
    "Click Create Bucket (or use an existing bucket).\n",
    "\n",
    "### Step 4\n",
    "In the Create a bucket window that will appear, add a unique bucket name and leave the remaining settings at their default values.\n",
    "\n",
    "### Step 5\n",
    "Click Create.\n",
    "\n",
    "### Step 6\n",
    "You now have a Cloud Storage Bucket which we will be using to store raw data for ingestion into Google BigQuery later and for storing Cloud Dataprep settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery Dataset to store Cloud Dataprep Output\n",
    "\n",
    "### Step 1\n",
    "Open BigQuery at https://console.cloud.google.com/bigquery.\n",
    "\n",
    "### Step 2\n",
    "In the left side bar, click on your project name.\n",
    "\n",
    "### Step 3\n",
    "Click CREATE DATASET.\n",
    "\n",
    "### Step 4\n",
    "For Dataset ID, type taxi_cab_reporting and select Create dataset.\n",
    "\n",
    "Now you have a new empty dataset that we can populate with tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Cloud Dataprep\n",
    "### Step 1\n",
    "Open the Navigation menu.\n",
    "\n",
    "### Step 2\n",
    "Under Big Data, click on Dataprep.\n",
    "\n",
    "### Step 3\n",
    "Agree to the Terms of Service.\n",
    "\n",
    "### Step 4\n",
    "Click Agree and Continue.\n",
    "\n",
    "5eabff1419ebfaea.png\n",
    "\n",
    "Click Allow for Trifacta to access project data. Dataprep is provided in collaboration with Trifacta, a Google partner. 59ddf08c1bbcf24b.png\n",
    "\n",
    "### Step 5\n",
    "Click Allow.\n",
    "\n",
    "18fc12676fc080ed.png\n",
    "\n",
    "### Step 6\n",
    "When prompted for \"First Time Setup\", click Continue.\n",
    "\n",
    "### Step 7\n",
    "Wait for Cloud Dataprep to initialize (less than a minute typically).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import NYC Taxi Data from GCS into a Dataprep Flow\n",
    "### Step 1\n",
    "In the Cloud Dataprep UI, click Create Flow.\n",
    "\n",
    "### Step 2\n",
    "Specify the following Flow details:\n",
    "\n",
    "|Flow Name|Flow Description|\n",
    "|:-|:-|\n",
    "|NYC Taxi Cab Data Reporting|Ingesting, Transforming, and Analyzing Taxi Data|\n",
    "\n",
    "Click Create.\n",
    "\n",
    "If prompted, dismiss the helper tutorial.\n",
    "\n",
    "### Step 3\n",
    "Click Import & Add Datasets.\n",
    "\n",
    "### Step 4\n",
    "In the data importer left side menu, click GCS (Google Cloud Storage).\n",
    "\n",
    "### Step 5\n",
    "Click the Pencil Icon to edit the GCS path.\n",
    "\n",
    "### Step 6\n",
    "Paste in the 2015 taxi rides dataset CSV from Google Cloud Storage:\n",
    "```\n",
    "gs://asl-ml-immersion/nyctaxicab/tlc_yellow_trips_2015.csv\n",
    "```\n",
    "Click Go.\n",
    "\n",
    "### Step 7\n",
    "Before selecting Import, click the Pencil Icon to edit the GCS path a second time and paste in the 2016 CSV below:\n",
    "```\n",
    "gs://asl-ml-immersion/nyctaxicab/tlc_yellow_trips_2016.csv\n",
    "```\n",
    "Click Go.\n",
    "\n",
    "### Step 8\n",
    "Click Import & Add to Flow.\n",
    "\n",
    "c84d286e0bf83367.png\n",
    "\n",
    "### Step 9\n",
    "Wait for the datasets to be loaded into DataPrep.\n",
    "\n",
    "The tool load a 10MB sample of the underlying data as well as connects to and ingests the original data source when the flow is ran.\n",
    "\n",
    "### Step 10\n",
    "Click on the tlc_yellow_trips_2015 icon and select Add New Recipe.\n",
    "\n",
    "739caa0b0fc2d860.png\n",
    "\n",
    "### Step 11\n",
    "Click Edit Recipe.\n",
    "\n",
    "Wait for Dataprep to load your data sample into the explorer view\n",
    "\n",
    "### Step 12\n",
    "In the explorer view, find the trip_distance column and examine the histogram.\n",
    "\n",
    "True or False, the majority of the cab rides for 2015 were less than 5 miles.\n",
    "\n",
    "f183c09abec52669.png\n",
    "\n",
    "True. In our sample, 68% were between 0 to 5 miles.\n",
    "\n",
    "Now, let's combine our 2016 and 2015 datasets.\n",
    "\n",
    "### Step 13\n",
    "In the navigation bar, find the icon for Union and select it.\n",
    "\n",
    "cc1a658aa4b1a32e.png\n",
    "\n",
    "### Step 14\n",
    "In the Union Page, click Add data.\n",
    "\n",
    "In the popup window, select tlc_yellow_trips_2016 and click Apply.\n",
    "\n",
    "### Step 15\n",
    "Confirm the union looks like below (UNION DATA (2)) and then click Add to Recipe.\n",
    "\n",
    "5275e266f3a7d849.png\n",
    "\n",
    "Wait for Dataprep to Apply the Union.\n",
    "\n",
    "Now we have a single table with 2016 and 2015 taxicab data.\n",
    "\n",
    "## Exploring your Data\n",
    "### Step 16\n",
    "Examine the pickup_time histogram. Which hours had the fewest amount of pickups? The most?\n",
    "\n",
    "In our sample, the early morning hours (3 - 4am) had the fewest taxicab pickups.\n",
    "\n",
    "8db12f8faf352ae8.png\n",
    "\n",
    "The most taxi cab pickups were in the evening hours with 21:00 (9pm) having slightly more than others.\n",
    "\n",
    "bfb3ac39d47e8a6e.png\n",
    "\n",
    "Is this unusual? Would you expect NYC taxi cab trips to be clustered around lunch and earlier hours in the day? Let's continue exploring.\n",
    "\n",
    "Examine the pickup_day histogram. Which months and years of data do we have in our dataset?\n",
    "\n",
    "Only December 2015 and December 2016\n",
    "360c1c8ce7196679.png\n",
    "\n",
    "Examine the dropoff_day histogram. Is there anything unusual about it when compared to pickup_day? Why are there records for January 2017?\n",
    "\n",
    "4ad948eeb3d7aba2.png\n",
    "\n",
    "Answer: There are quite a few trips that start in December and end in January (spending New Years in a taxicab!).\n",
    "\n",
    "Next, we want to concatenate our date and time fields into a single timestamp.\n",
    "\n",
    "### Step 17\n",
    "In the navigation bar, find Merge columns.\n",
    "\n",
    "864cafbb906a2cca.png\n",
    "\n",
    "For columns to merge, specify pickup_day and pickup_time.\n",
    "\n",
    "For separator type a single space.\n",
    "\n",
    "Name the new column pickup_datetime.\n",
    "\n",
    "Preview and click Add.\n",
    "\n",
    "d6a4d0d6a9a348ef.png\n",
    "\n",
    "Confirm your new field is properly registering now as a datetime datatype (clock icon).\n",
    "\n",
    "9273009ca2303d8b.png\n",
    "\n",
    "### Step 18\n",
    "Next, we want to create a new derived column to count the average amount of passengers in the last hour. To do that, we need to create to get hourly data and perform a calculation.\n",
    "\n",
    "Find the Functions list in the navigation bar.\n",
    "\n",
    "Select Dates and times.\n",
    "\n",
    "Select DATEFORMAT.\n",
    "\n",
    "9f634021f028bd3f.png\n",
    "\n",
    "In the formula, paste the following which will truncate the pickup time to just the hour:\n",
    "```\n",
    "DATEFORMAT(pickup_datetime,\"yyyyMMddHH\")\n",
    "```\n",
    "8a396bfb77d9854e.png\n",
    "\n",
    "Specify the New Column as hour_pickup_datetime.\n",
    "\n",
    "Confirm the new derived column is shown correctly in the preview.\n",
    "\n",
    "91d6c220515efcc1.png\n",
    "\n",
    "Click Add.\n",
    "\n",
    "### Step 19\n",
    "In order to get the field properly recognized as a DATETIME data type, we are going to add back zero minutes and zero seconds through a MERGE concatenation.\n",
    "\n",
    "In the navigation bar, find Merge columns.\n",
    "\n",
    "864cafbb906a2cca\n",
    "\n",
    "For columns to merge, specify hour_pickup_datetime and '0000'.\n",
    "\n",
    "Name the column to pickup_hour.\n",
    "\n",
    "811495c4ff7e9022.png\n",
    "\n",
    "Click Add.\n",
    "\n",
    "We now have our taxicab hourly pickup column. Next, we will calculate the average count of passengers over the past hour. We will do this through aggregations and a rolling window average function.\n",
    "\n",
    "### Step 20\n",
    "In the navigation toolbar select Functions > Aggregation > AVERAGE.\n",
    "\n",
    "ad1bb11a7fba20b0.png\n",
    "\n",
    "For Formula, specify:\n",
    "```\n",
    "AVERAGE(fare_amount)\n",
    "```\n",
    "For Sort rows, specify:\n",
    "```\n",
    "pickup_datetime\n",
    "```\n",
    "For Group by, specify:\n",
    "```\n",
    "pickup_hour\n",
    "```\n",
    "7f4646dc02c251c9.png\n",
    "\n",
    "Click Add.\n",
    "\n",
    "We now have our average cab fares statistic.\n",
    "\n",
    "### Step 21\n",
    "Explore the average_fare_amount histogram. Is there a range of fares that are most common?\n",
    "\n",
    "db1b0979177de255.png\n",
    "\n",
    "In our sample, most NYC cab fares are in the $18-19 range.\n",
    "\n",
    "Next, we want to calculate a rolling window of average fares over the past 3 hours.\n",
    "\n",
    "### Step 22\n",
    "In the navigation toolbar, select Functions > Window > ROLLINGAVERAGE.\n",
    "\n",
    "3ba77249ce0b96b1.png\n",
    "\n",
    "Copy in the below formula which computes the rolling average of passenger count for the last hour.\n",
    "\n",
    "Formula:\n",
    "```\n",
    "ROLLINGAVERAGE(average_fare_amount, 3, 0)\n",
    "```\n",
    "Sort rows by:\n",
    "```\n",
    "-pickup_hour\n",
    "```\n",
    "Note that we are sorting recent taxicab rides first (the negative sign -pickup_hour indicates descending order) and operating over a rolling 3 hour period.\n",
    "\n",
    "5d65593d8ab10586.png\n",
    "\n",
    "Click Add.\n",
    "\n",
    "### Step 23\n",
    "Toggle open the Recipe icon to preview your final transformation steps.\n",
    "\n",
    "ac7bfdfafab6e41.png\n",
    "\n",
    "775040262524cfd8.png\n",
    "\n",
    "### Step 24\n",
    "Click Run Job.\n",
    "\n",
    "### Step 25\n",
    "In Publishing Actions page, under Settings, edit the path by clicking the pencil icon\n",
    "\n",
    "770160f86eb41a96.png\n",
    "\n",
    "Choose BigQuery and choose your taxi_cab_reporting BigQuery dataset where you want to create the output table.\n",
    "\n",
    "(Note: if you do not see a taxi_cab_reporting dataset, refer to the start of this lab for instructions on how to create it in BigQuery)\n",
    "\n",
    "Choose Create a new table.\n",
    "\n",
    "Name the table tlc_yellow_trips_reporting.\n",
    "\n",
    "Choose Drop the table every run.\n",
    "\n",
    "Select Update.\n",
    "\n",
    "### Step 26\n",
    "Select Run Job.\n",
    "\n",
    "### Step 27\n",
    "Optional: View the Cloud Dataflow Job by selecting [...] and View dataflow job.\n",
    "\n",
    "64c9d114b1186328.png\n",
    "\n",
    "Wait for your Cloud Dataflow job to complete and confirm your new new table shows in BigQuery.\n",
    "\n",
    "### Step 28\n",
    "While your Cloud Dataprep flow starts and manages your Cloud Dataflow job, you can see the data results by running this pre-ran query in BigQuery:\n",
    "```\n",
    "#standardSQL\n",
    "SELECT\n",
    "  pickup_hour,\n",
    "  FORMAT(\"$%.2f\",ROUND(average_3hr_rolling_fare,2)) AS avg_recent_fare,\n",
    "  ROUND(average_trip_distance,2) AS average_trip_distance_miles,\n",
    "  FORMAT(\"%'d\",sum_passenger_count) AS total_passengers_by_hour\n",
    "FROM\n",
    "  `asl-ml-immersion.demo.nyc_taxi_reporting`\n",
    "ORDER BY\n",
    "  pickup_hour DESC;\n",
    "```\n",
    "Extra credit:\n",
    "\n",
    "You can schedule Cloud Dataprep jobs to run at set intervals. Select a flow and click [...] and Schedule Flow.\n",
    "\n",
    "schedule_flow.png\n",
    "\n",
    "Congratulations! You have now built a data transformation pipeline using the Cloud Dataprep UI.\n",
    "\n",
    "For full documentation and additional tutorials, refer to the [Cloud Dataprep support page](https://cloud.google.com/dataprep/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End your lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
